# store column y as a factor
spam$y <- factor(spam$y)
#Normalize
num.vars <- sapply(spam, is.numeric)
spam[num.vars] <- lapply(spam[num.vars], scale)
#Divide dataset into train and test
set.seed(4601)
test <- 1:1000
train.spam <- spam.subset[-test,]
test.spam <- spam.subset[test,]
train.def <- spam$y[-test]
test.def <- spam$y[test]
library(class)
library(caret)
library(e1071)
prostate_Cancer <- read.csv("Prostate_Cancer.csv")
## Convert the dependent var to factor. Normalize the numeric variables
num.vars <- sapply(prostate_Cancer, is.numeric)
prostate_Cancer[num.vars] <- lapply(prostate_Cancer[num.vars], scale)
## Selecting only 3 numeric variables for this demostration, just to keep things simple
myvars <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "symmetry", "fractal_dimension")
prostate_Cancer.subset  <- prostate_Cancer[myvars]
summary(prostate_Cancer[myvars])
## Let's predict on a test set of 100 observations. Rest to be used as train set.
set.seed(100)
test <- 1:20
train.prostate_Cancer <- prostate_Cancer.subset[-test,]
test.prostate_Cancer <- prostate_Cancer.subset[test,]
train.def <- prostate_Cancer$diagnosis_result[-test]
test.def <- prostate_Cancer$diagnosis_result[test]
library(class)
library(caret)
library(e1071)
library(class)
library(caret)
library(e1071)
# Load Data from source into a spam dataframe
spam <- read_xlsx("Spam_filter.xlsx")
# nrow return the number of rows in spam
# sample returns Random Samples and Permutations in spam
# spam[sample(nrow(spam)),] return all rows and columns
spam <- spam[sample(nrow(spam)),]
# use factors for norminal values
# store column y as a factor
spam$y <- factor(spam$y)
#Normalize
num.vars <- sapply(spam, is.numeric)
spam[num.vars] <- lapply(spam[num.vars], scale)
#Divide dataset into train and test
set.seed(4601)
test <- 1:1000
train.spam <- spam.subset[-test,]
test.spam <- spam.subset[test,]
train.def <- spam$y[-test]
test.def <- spam$y[test]
num.vars <- sapply(spam, is.numeric)
num.vars
colnames(spam)
myvars <-colnames(spam)
myvars
str(myvars)
str(myvars[1:57])
#Normalize
num.vars <- sapply(spam, is.numeric)
spam[num.vars] <- lapply(spam[num.vars], scale)
myvars <-colnames(spam)
#myvars <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "symmetry", "fractal_dimension")
spam.subset  <- spam[myvars[1:57]]
spam.subset
num.vars <- sapply(spam, is.numeric)
spam[num.vars] <- lapply(spam[num.vars], scale)
myvars <-colnames(spam)
#myvars <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "symmetry", "fractal_dimension")
spam.subset  <- spam[myvars[1:57]]
#Divide dataset into train and test
set.seed(4601)
test <- 1:1000
train.spam <- spam.subset[-test,]
test.spam <- spam.subset[test,]
train.def <- spam$y[-test]
test.def <- spam$y[test]
library(class)
library(readxl)
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
#Use KNN algorithm
knn.1 <- knn(train.spam, test.spam, train.def, k=1)
knn.3 <- knn(train.spam, test.spam, train.def, k=3)
knn.5 <- knn(train.spam, test.spam, train.def, k=5)
knn.8 <- knn(train.spam, test.spam, train.def, k=8)
knn.10 <- knn(train.spam, test.spam, train.def, k=10)
knn.20 <- knn(train.spam, test.spam, train.def, k=20)
#Calculate correct classification (accuracy)
table(knn.1, test.def)
table(knn.3, test.def)
table(knn.5, test.def)
table(knn.8, test.def)
table(knn.10, test.def)
table(knn.20, test.def)
confusionMatrix(knn.1, test.def)
confusionMatrix(knn.3, test.def)
confusionMatrix(knn.5, test.def)
confusionMatrix(knn.8, test.def)
confusionMatrix(knn.10, test.def)
confusionMatrix(knn.20, test.def)
confusionMatrix(knn.1, test.def)
confusionMatrix(knn.5, test.def)
confusionMatrix(knn.20, test.def)
library(class)
library(caret)
library(e1071)
prostate_Cancer <- read.csv("Prostate_Cancer.csv")
## Convert the dependent var to factor. Normalize the numeric variables
num.vars <- sapply(prostate_Cancer, is.numeric)
prostate_Cancer[num.vars] <- lapply(prostate_Cancer[num.vars], scale)
## Selecting only 3 numeric variables for this demostration, just to keep things simple
myvars <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "symmetry", "fractal_dimension")
prostate_Cancer.subset  <- prostate_Cancer[myvars]
summary(prostate_Cancer[myvars])
## Let's predict on a test set of 100 observations. Rest to be used as train set.
set.seed(100)
test <- 1:20
train.prostate_Cancer <- prostate_Cancer.subset[-test,]
test.prostate_Cancer <- prostate_Cancer.subset[test,]
train.def <- prostate_Cancer$diagnosis_result[-test]
test.def <- prostate_Cancer$diagnosis_result[test]
## Let's use k values (no of NNs) as 1, 5 and 20 to see how they perform in terms of correct proportion of classification and success rate. The optimum k value can be chosen based on the outcomes as below...
knn.1 <-  knn(train.prostate_Cancer, test.prostate_Cancer, train.def, k=1)
knn.5 <-  knn(train.prostate_Cancer, test.prostate_Cancer, train.def, k=5)
knn.20 <- knn(train.prostate_Cancer, test.prostate_Cancer, train.def, k=20)
## Let's calculate the proportion of correct classification for k = 1, 5 & 20
100 * sum(test.def == knn.1)/100  # For knn = 1
100 * sum(test.def == knn.5)/100  # For knn = 5
100 * sum(test.def == knn.20)/100 # For knn = 20
## We should also look at the success rate against the value of increasing K.
table(knn.1 ,test.def)
table(knn.5 ,test.def)
table(knn.20 ,test.def)
## Evaluation of the predictive performance
confusionMatrix(knn.1, test.def)
confusionMatrix(knn.5, test.def)
confusionMatrix(knn.20, test.def)
install.packages("ElemStatLearn")
View(train.prostate_Cancer)
View(train.prostate_Cancer)
View(train.prostate_Cancer)
View(train.prostate_Cancer)
library(ElemStatLearn)
set = train.prostate_Cancer
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = train.prostate_Cancer[, -3],
test = grid_set,
cl = train.prostate_Cancer[, 3],
k = 5)
plot(set[, -3],
main = 'K-NN (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = train.prostate_Cancer
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = train.prostate_Cancer[, -3],
test = grid_set,
cl = train.prostate_Cancer[, 3],
k = 5)
plot(set[, -3],
main = 'K-NN (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
View(train.prostate_Cancer)
View(train.prostate_Cancer)
View(train.prostate_Cancer)
View(train.prostate_Cancer)
set = train.prostate_Cancer
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Radius', 'Texture')
y_grid = knn(train = train.prostate_Cancer,
test = grid_set,
cl = train.prostate_Cancer,
k = 5)
plot(set[, -3],
main = 'K-NN (Training set)',
xlab = 'Radius', ylab = 'Texture',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set == 1, 'green4', 'red3'))
set = train.prostate_Cancer
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Radius', 'Texture')
y_grid = knn(train = train.prostate_Cancer,
test = grid_set,
cl = train.prostate_Cancer,
k = 5)
plot(set,
main = 'K-NN (Training set)',
xlab = 'Radius', ylab = 'Texture',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set == 1, 'green4', 'red3'))
set = train.prostate_Cancer
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Radius', 'Texture')
y_grid = knn(train = train.prostate_Cancer,
test = grid_set,
cl = train.prostate_Cancer,
k = 5)
plot(set,
main = 'K-NN (Training set)',
xlab = 'Radius', ylab = 'Texture',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set == 1, 'green4', 'red3'))
View(train.prostate_Cancer)
View(train.prostate_Cancer)
library(class)
library(readxl)
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
# Load Data from source into a spam dataframe
spam <- read_xlsx("Spam_filter.xlsx")
# nrow return the number of rows in spam
# sample returns Random Samples and Permutations in spam
# spam[sample(nrow(spam)),] return all rows and columns
spam <- spam[sample(nrow(spam)),]
# use factors for norminal values
# store column y as a factor
spam$y <- factor(spam$y)
#Normalize
num.vars <- sapply(spam, is.numeric)
spam[num.vars] <- lapply(spam[num.vars], scale)
myvars <-colnames(spam)
spam.subset  <- spam[myvars[1:57]]
#Divide dataset into train and test
set.seed(4601)
test <- 1:1000
train.spam <- spam.subset[-test,]
test.spam <- spam.subset[test,]
train.def <- spam$y[-test]
test.def <- spam$y[test]
#Use KNN algorithm
knn.1 <- knn(train.spam, test.spam, train.def, k=1)
knn.3 <- knn(train.spam, test.spam, train.def, k=3)
knn.5 <- knn(train.spam, test.spam, train.def, k=5)
knn.8 <- knn(train.spam, test.spam, train.def, k=8)
knn.10 <- knn(train.spam, test.spam, train.def, k=10)
knn.20 <- knn(train.spam, test.spam, train.def, k=20)
#Calculate correct classification (accuracy)
table(knn.1, test.def)
table(knn.3, test.def)
table(knn.5, test.def)
table(knn.8, test.def)
table(knn.10, test.def)
table(knn.20, test.def)
confusionMatrix(knn.1, test.def)
confusionMatrix(knn.3, test.def)
confusionMatrix(knn.5, test.def)
confusionMatrix(knn.8, test.def)
confusionMatrix(knn.10, test.def)
confusionMatrix(knn.20, test.def)
student.physics.marks <- c( 70L , 75L , 80L,  85L)
student.chemistry.marks <- c(60L, 70L, 85L, 70L)
student.marks <-cbind(student.physics.marks , student.chemistry.marks)
rownames(student.marks) <- c("Raj","Rahul","Priya","Poonam")
student.marks
student.marks
install.packages("mlbench")
library(mlbench)
data("HouseVotes84")
View(HouseVotes84)
View(HouseVotes84)
housevotes <- data("HouseVotes84")
View(housevotes)
housevotes <- data("HouseVotes84")
head(housevotes)
na_by_col_clas s <-í°€
function ( col , c l s ){
return (sum(
i s . na(
HouseVotes84 [ , col ]
) &
HouseVotes84$Class==c l s
)
)
}
na_by_col_class <-í°€
function ( col , cls ){
return (sum(
is.na(
HouseVotes84[ , col ]
) &
HouseVotes84$Class==cls
)
)
}
na_by_col_class
na_by_col_class <-í°€
function ( col , cls ){
return (sum(
is.na(
HouseVotes84[ , col ]
) &
HouseVotes84$Class==cls
)
)
}
na_by_col_class
na_by_col_class <-í°€
function ( col , cls ){
return (sum(
is.na(
HouseVotes84[ , col ]
) &
HouseVotes84$Class==cls
)
)
}
na_by_col_class("Class", "republican")
na_by_col_class()
dataset.class <- as.numeric(dataset$Class)
dataset <- data("HouseVotes84")
dataset.class <- as.numeric(dataset$Class)
dataset <- data("HouseVotes84")
dataset <- as.data.frame("HouseVotes84")
dataset.class <- as.numeric(dataset$Class)
hist(data.class)
dataset <- as.data.frame("HouseVotes84")
dataset.class <- as.numeric(dataset$Class)
hist(dataset.class)
dataset <- as.data.frame("HouseVotes84")
dataset.class <- as.numeric(dataset$Class)
hist(dataset.class)
dataset.class <- as.numeric(dataset$Class)
dataset <- as.data.frame(HouseVotes84)
dataset.class <- as.numeric(dataset$Class)
hist(dataset.class)
View(HouseVotes84)
View(HouseVotes84)
library(e1071)
library(e1071)
library("mlbench")
dataset <- as.data.frame(unlist(HouseVotes84))
hist(dataset.class, col=c("Blue", "Red"), breaks=2)
#WTF
na_by_col_class <- function (col,cls){
return (sum(
is.na(
HouseVotes84[,col]
) &
HouseVotes84$Class==cls
)
)
}
na_by_col_class(3, "democrat")
View(HouseVotes84)
HouseVotes84[,"train"] <- ifelse(
runif(nrow(HouseVotes84))
<0.80,1,0
)
#WTF
na_by_col_class <- function (col,cls){
return (sum(
is.na(
HouseVotes84[,col]
) &
HouseVotes84$Class==cls
)
)
}
na_by_col_class(3, "democrat")
View(HouseVotes84)
HouseVotes84[,"train"] <- ifelse(
runif(nrow(HouseVotes84))
<0.80,1,0
)
trainColNum <- grep("train", names(HouseVotes84))
trainHouseVotes84 <- HouseVotes84[HouseVotes84$train==1,-trainColNum]
testHouseVotes84 <- HouseVotes84[HouseVotes84$train==0,-trainColNum]
library(e1071)
model <- naiveBayes(Class~., trainHouseVotes84)
prediction <-predict(model, testHouseVotes84)
confusionMatrix(testHouseVotes84$Class, prediction)
model <- naiveBayes(Class~., trainHouseVotes84)
prediction <-predict(model, testHouseVotes84)
confusionMatrix(testHouseVotes84$Class, prediction)
model <- naiveBayes(Class~., trainHouseVotes84)
prediction <-predict(model, testHouseVotes84)
confusionMatrix(testHouseVotes84$Class, prediction)
View(trainHouseVotes84)
View(trainHouseVotes84)
View(testHouseVotes84)
View(testHouseVotes84)
summary(model)
summary(trainHouseVotes84)
str(trainHouseVotes84)
#load mlbench library
library(mlbench)
setwd("D:\Workspace\Data_science\NaiveBayes")
setwd("D:/Workspace/Data_science/NaiveBayes")
data(â€œHouseVotes84â€)
data("HouseVotes84")
#load mlbench library
library(mlbench)
#set working directory if needed (modify path as needed)
setwd("D:/Workspace/Data_science/NaiveBayes")
data("HouseVotes84")
plot(as.factor(HouseVotes84[,2]))
title(main="Votes cast for issue", xlab="vote", ylab="# reps")
#by party
plot(as.factor(HouseVotes84[HouseVotes84$Class==â€™republicanâ€™,2]))
title(main="Republican votes cast for issue 1â€³, xlab="vote", ylab="# reps")
plot(as.factor(HouseVotes84[HouseVotes84$Class==â€™democratâ€™,2]))
title(main="Democrat votes cast for issue 1â€³, xlab="vote", ylab="# reps")
title(main="Votes cast for issue", xlab="vote", ylab="# reps")
#by party
plot(as.factor(HouseVotes84[HouseVotes84$Class=="republican",2]))
title(main="Republican votes cast for issue 1â€³, xlab="vote", ylab="# reps")
plot(as.factor(HouseVotes84[HouseVotes84$Class=="democrat",2]))
title(main="Democrat votes cast for issue 1", xlab="vote", ylab="# reps")
plot(as.factor(HouseVotes84[HouseVotes84$Class=='republican',2]))
plot(as.factor(HouseVotes84[HouseVotes84$Class=='democrat',2]))
plot(as.factor(HouseVotes84[,2]))
title(main="Votes cast for issue", xlab="vote", ylab="# reps")
plot(as.factor(HouseVotes84[HouseVotes84$Class=='republican',2]))
title(main="Republican votes cast for issue 1â€³, xlab="vote", ylab="# reps")
#barplots for specific issue
plot(as.factor(HouseVotes84[,2]))
title(main="Votes cast for issue", xlab="vote", ylab="# reps")
#by party
plot(as.factor(HouseVotes84[HouseVotes84$Class=='republican',2]))
title(main="Republican votes cast for issue 1â€³, xlab="vote", ylab="# reps")
#by party
plot(as.factor(HouseVotes84[HouseVotes84$Class=='republican',2]))
title(main="Republican votes cast for issue 1", xlab="vote", ylab="# reps")
plot(as.factor(HouseVotes84[HouseVotes84$Class=='democrat',2]))
title(main="Democrat votes cast for issue 1", xlab="vote", ylab="# reps")
#Functions needed for imputation
#function to return number of NAs by vote and class (democrat or republican)
na_by_col_class <- function (col,cls){return(sum(is.na(HouseVotes84[,col]) & HouseVotes84$Class==cls))}
#function to compute the conditional probability that a member of a party will cast a â€˜yesâ€™ vote for
#a particular issue. The probability is based on all members of the party who #actually cast a vote on the issue (ignores NAs).
p_y_col_class <- function(col,cls){
sum_y<-sum(HouseVotes84[,col]=='y' & HouseVotes84$Class==cls,na.rm = TRUE)
sum_n<-sum(HouseVotes84[,col]=='n' & HouseVotes84$Class==cls,na.rm = TRUE)
return(sum_y/(sum_y+sum_n))}
#Check that functions work!
p_y_col_class(2,'democrat')
p_y_col_class(2,'republican'')
na_by_col_class(2,'democrat')
na_by_col_class(2,'republican'')
p_y_col_class(2,'democrat')
p_y_col_class(2,'democrat')
data("HouseVotes84")
p_y_col_class(2,'democrat')
na_by_col_class(2,'democrat')
na_by_col_class(2,'republican') # Returns the number of NAs for R
View(HouseVotes84)
View(HouseVotes84)
for (i in 2:ncol(HouseVotes84)) {
if(sum(is.na(HouseVotes84[,i])>0)) {
c1 <- which(is.na(HouseVotes84[,i])& HouseVotes84$Class=='democrat',arr.ind = TRUE)
c2 <- which(is.na(HouseVotes84[,i])& HouseVotes84$Class=='republican',arr.ind = TRUE)
HouseVotes84[c1,i] <- ifelse(runif(na_by_col_class(i,'democrat'))<p_y_col_class(i,'democrat'),'y','n')
HouseVotes84[c2,i] <- ifelse(runif(na_by_col_class(i,'republican'))<p_y_col_class(i,'republican'),'y','n')}
}
#divide into test and training sets
#create new col â€œtrainâ€ and assign 1 or 0 in 80/20 proportion via random uniform dist
HouseVotes84[,"train"] <- ifelse(runif(nrow(HouseVotes84))<0.80,1,0)
#get col number of train / test indicator column (needed later)
trainColNum <- grep(â€œtrainâ€,names(HouseVotes84))
#separate training and test sets and remove training column before modeling
trainHouseVotes84 <- HouseVotes84[HouseVotes84$train==1,-trainColNum]
testHouseVotes84 <- HouseVotes84[HouseVotes84$train==0,-trainColNum]
#divide into test and training sets
#create new col â€œtrainâ€ and assign 1 or 0 in 80/20 proportion via random uniform dist
HouseVotes84[,"train"] <- ifelse(runif(nrow(HouseVotes84))<0.80,1,0)
#get col number of train / test indicator column (needed later)
trainColNum <- grep("train",names(HouseVotes84))
#separate training and test sets and remove training column before modeling
trainHouseVotes84 <- HouseVotes84[HouseVotes84$train==1,-trainColNum]
testHouseVotes84 <- HouseVotes84[HouseVotes84$train==0,-trainColNum]
library(e1071)
nb_model <- naiveBayes(Class~.,data = trainHouseVotes84)
nb_model
summary(nb_model)
str(nb_model)
nb_model
nb_model
